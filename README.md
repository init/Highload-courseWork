### 1. Выбор темы
* Яндекс.Музыка

### 2. Определение возможного диапазона нагрузок

* Месячная аудитория [15 млн](https://vc.ru/services/98608-ot-kataloga-i-poiska-do-personalnogo-muzykalnogo-pomoshchnika-kak-izmenilas-yandeks-muzyka-i-chto-ee-zhdet-v-budushchem) 

* Дневная аудитория [1.9 млн](https://radar.yandex.ru/top_list?thematic=culture%2Cmusic)

* Пользователи в среднем слушают музыку [90](https://vc.ru/media/96460-chislo-podpischikov-yandeks-muzyki-vyroslo-v-tri-raza-za-poltora-goda-i-dostiglo-3-mln) минут в день

### 3. Выбор планируемой нагрузки как 60% доля рынка в России

Самыми большими по количество платных подписчиков являются ["Яндекс.Музыка", Boom/VK Music, Apple Music](https://www.dp.ru/a/2019/05/17/Muzikalnaja_strana__Rinku)                        
(3 млн, 1.2 млн, 0.7 млн соответственно) поэтому планируемая нагрузка 60% всего рынка

За час прослушивания музыки расходуется [85Mb](https://yandex.ru/support/music-app-winmobile/search-and-listen/cost.html) 
трафика. В час пользователей примерно 1.9млн/24 = 80 000. Таким образом, в час нагрузка 80 000 * 85Mb = 6.8Tb.
В минуту будет следующая нагрузка на одного человека 1.42Mb. 1319 человек в минуту. Значит 1319 * 1.42Mb = 1.9Gb 
в минуту. Пользователь слушает в день 90 мин -> 126Mb. Дневная аудитория 1.9 млн -> 243.2 Tb -- _дневная нагрузка_.

### 4. Логическая схема базы данных (без выбора СУБД)
![](./pic/Untitled.png)

### 5. Физическая системы хранения (конкретные СУБД, шардинг, расчет нагрузки, обоснование реализуемости на основе результатов нагрузочного тестирования)
``` 
1 900 0000 активных пользователей
x
90 минут
/
24 часа x 60 минут
------------------------------------------
~ 120 000 RPS
``` 
Хранение - это в первую очередь PostgreSQL (справится и один, но лучше несколько), Cassandra или memcached (различные статические индексы). Последние нужны
для различных контент- сервисов, например для поиска или метаданных. Аудиофайлы хранятся в Amazon S3 и кэшируются в
нашем бэкенде или с помощью CDNs для низкой задержки.

Нет смысла партицировать таблицы по какому-то одному признаку, лучше предоставить право выбора способа 
партицирования/шардирования каждой из команд разработчиков, команда должна реализовать сегментацию самостоятельно
в своих сервисах, однако многие сервисы полагаются на то, что Кассандра делает полные реплики данных между сайтами.
Настройка полного кластера хранения данных с репликацией и отказоустойчивостью между сайтами является сложной задачей,
поэтому лучше всего создать  инфраструктуру для настройки и обслуживания мультисайтовых кластеров Cassandra или
postgreSQL как единого целого.Хранилище будет ограничено очень простым хранилищем ключей и значений. Для этого
подойдет Dynamo по мотивам DHTs. Таким образом, у каждого запроса будет ключ;  каждый сервисный узел отвечает за
диапазон хэш- ключей;  данные распределяются между сервисными узлами; избыточность обеспечивается записью в узел реплики. 

### 6. Выбор прочих технологий: языки программирования, фреймфорки, протоколы взаимодействия, веб-сервера и т.д. (с обоcнованием выбора)

_web client_: JavaScrip.

_backend_: Чтобы JavaScript мог взаимодействовать с бэкэндом, можно организовать высоко оптимизированную службу C++(*),
которая может обрабатывать множество активных соединений одновременно.Эта служба будет отвечать за маршрутизацию 
запросов к правильному бэку. Эта служба может работать через порты 80 и 443 для преодоления ограничений 
брандмауэра. Связь осуществляется через WebSocket (или Flash для некоторых браузеров). Для связи с конкретными бэкенд-
сервисами мы направляем запросы через(*), используя  ZeroMQ и Protobuf. Сами бэки пишутся на Go или Python.

_app for smartphone_: Java для android, ObjC для ios.

### 7. Расчет нагрузки и потребного оборудования

У нас есть следующие узлы : фронтенд; бэкенд; сервера, которые хранят музыку; балансировщик(Software Load Balancers); 
сборщик статистики.

_Фронтенд_: 

объем фронта пример [3 мб](https://habr.com/ru/company/tinkoff/blog/474632/) ->
3 * 80 000(количество пользователей в час) = 240 гб/ч возьмем 64ГБ памяти 32 ядра SSD без RAID 10G Ethernet(передавать 
Ethernet пакеты со скоростью 10 гигабит в секунду) возьмем два сервера  

_Бэкенд_:

128Гб памяти 32 ядра SATA диск без RAID 10G Ethernet с нагрузкой, рассчитанной в пункте 3, должно хватить 6 серверов
(с запасом)

_БД_ :

128Гб памяти 32 ядра SSD RAID6 10G Ethernet
берем 2 для хранения информации о пользователях 

_Сервер для музыки_ :

Положим, что вес всех песен 100Тб, таким образом количество необходимых серверов при ежедневной нагрузке и при значениях 
трафика для скачивания песен 15 серверов со следующей конфигурацией: 
2ТБ * 4(То есть параллельно работают 4 модуля или четыре пары модулей) разъемов памяти 32 ядра SATA диск RAID6 10G Ethernet


_Nginx(Software Load Balancers)_ :
Возьмем 64Гб памяти 32 ядра SATA без RAID 10G Ethernet. 2 для бэка и 30 для серверов с музыкой для обеспечения максимальной 
доступности

_Сборщик статиски_ :

8Гб памяти 32 ядра SATA без RAID 10G Ethernet - 2шт


### 8. Выбор хостинга / облачного провайдера и расположения серверов
Можно двумя способами организовать место хранения данных: в собственных центрах обработки данных получится меньшая задержка
и более стабильная среда. В общедоступном облаке мы получаем гораздо более быструю подготовку аппаратного обеспечения 
и гораздо более динамичные возможности масштабирования. Воспользуемся Google Cloud Platform (GCP). Присутствуют центры в европе, кроме этого имеет следующие преимущества:

* Частная глобальная волоконная сеть
* Динамическая миграция виртуальных машин
* Повышение производительности
* Современная безопасность
* Стремительно наращивает свою инфраструктуру для облачной платформы
* Избыточное резервное копирование

### 9. Схема балансировки нагрузки (входящего трафика и внутрипроектного, терминация SSL)
Балансируем нагрузку с помощью Nginx L7, терминация SSL -> Nginx

### 10. Обеспечение отказоустойчивости
Для обеспечения отказоустойчивости для каждого сервера БД храним две репликации (Master-Slave(2)). Таким образом Мастер 
сервер отвечает за изменения данных, а Слейв за чтение. Асинхронность репликации означает, что данные на Слейве могут 
появится с небольшой задержкой. Поэтому, в последовательных операциях необходимо использовать чтение с Мастера, чтобы 
получить актуальные данные. При выходе из строя Слейва, достаточно просто переключить все приложение на работу с Мастером. 
После этого восстановить репликацию на Слейве и снова его запустить.
Если выходит из строя Мастер, нужно переключить все операции (и чтения и записи) на Слейв. Таким образом он станет новым 
Мастером. После восстановления старого Мастера, настроить на нем реплику, и он станет новым Слейвом.

Кроме этого, собираем статистику о сервера и мониторим его. Grafana + Prometeus
